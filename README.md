# Introduction

This file is discribes the methodology used to perform the experiments presented in: **Resampling strategies for imbalanced regression: a survey**.

# Contents
This file contains:
- **code.ipynb** with the code implemented.
- **appendices** with all the results:
  - **Appendix A**: Results by dataset for better configuration of learning models and pre-processing strategies.
  - **Appendix B**: Average ranking for each metric.
  - **Appendix C**: Average number of training examples generated by each preprocessing strategy.
- **data** with the 17 datasets. The main characteristics of the data are:


|Datasets | N    | p.total | p.nom | p.num | nRaro | % Raro|
| -----------------|------|---------|--------|------|-------|-------|
|Abalone           |4177  | 8       | 1      | 7    | 679   | 16,3  |
|a3                |198   | 11      | 3      | 8    | 32    | 16,2  |
|a6                |198   | 11      | 3      | 8    | 32    | 16,2  |
|a4                |198   | 11      | 3      | 8    | 30    | 15,2  |
|a1                |198   | 11      | 3      | 8    | 28    | 14,1  |
|a7                |198   | 11      | 3      | 8    | 27    | 13,6  |
|boston            |506   | 13      | 0      | 13   | 65    | 12,8  |
|a2                |198   | 11      | 3      | 8    | 22    | 11,1  |
|a5                |198   | 11      | 3      | 8    | 21    | 10,7  |
|fuelCons          |1764  | 38      | 12     | 26   | 164   | 9,3   |
|heat              |7400  | 11      | 3      | 8    | 664   | 9,0   |
|availPwr          |1802  | 16      | 7      | 9    | 157   | 8,7   |
|cpuSm             |8192  | 13      | 0      | 13   | 713   | 8,7   |
|maxTorq           |1802  | 33      | 13     | 20   | 129   | 7,2   |
|ConcrStr          |1030  | 8       | 0      | 8    | 55    | 5,3   |
|Accel             |1732  | 15      | 3      | 12   | 89    | 5,1   |
|airfold           |1503  | 5       | 0      | 5    | 62    | 4,1   |

where, (N: number of cases; p.total: number of attributes; p.nom: number of nominal attributes; p.num: number of numeric attributes; nRare: number of rare cases; %Rare: 100 × NRaro/N )).


# Tools

The experiments were performed in Python and used packages in R through the library [rpy2](https://rpy2.github.io/).

- Packages:

  - [Scikit-learn](https://scikit-learn.org/stable/)
    - BaggingRegressor
    - DecisionTreeRegressor
    - MLPRegressor
    - RandomForestRegressor
    - SVR
  - [XGBoost](https://xgboost.readthedocs.io/)
    - XGBoost
  - [smogn 0.1.2](https://pypi.org/project/smogn/)
    - SMOGN
  - [resreg 0.2](https://pypi.org/project/resreg/)
    - WERCS
  - [UBL](https://github.com/paobranco/UBL)
    - SMOTER
    - Random over-sampling 
    - Random under-sampling
    - Gaussian Noise Introduction


- Metrics:
  - [F1-score](https://github.com/rpribeiro/uba)
  - [SERA](https://github.com/nunompmoniz/IRon)


The image describes the steps taken. The nominal attributes were codified transforming the vetor of categories into whole values between 0 and the number of categories−1, as for the ordinal attributes, a pre-defined order was established (ex. small: 1, medium: 2, large: 3). Then the training data set (_Train set_) and test set (_Test set_) were separated, the training data passed through the resampling process (_Resampling_), from the balanced data (_Balanced train set_) the learning model was generated (_Model generation_) that is evaluated (_Model evaluating_) and obtained its performance (_Performance estimation_).

![alt text](https://github.com/JusciAvelino/imbalancedRegression/blob/main/diagram.png)
